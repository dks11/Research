{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Research",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNwT6RT4PC2WaFi/XxpGu+S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dks11/Research/blob/main/Research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --pre flwr[simulation] torch torchvision matplotlib transformers datasets"
      ],
      "metadata": {
        "id": "wJmDsjcIkCkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "from typing import List\n",
        "from datetime import datetime\n",
        "\n",
        "import flwr as fl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.optim import AdamW\n",
        "from datasets import load_dataset, load_metric, Dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from transformers import AutoModelForSequenceClassification\n"
      ],
      "metadata": {
        "id": "5s9IxrcvjXwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "CHECKPOINT = \"bert-base-uncased\"  # transformer model checkpoint\n",
        "\n",
        "\n",
        "NUM_CLIENTS = 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "85PTl7fFjjlu",
        "outputId": "5b5b7a63-fc88-47d1-b76c-a41c465dcfaa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bd86847a6692>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mDEVICE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1ZxGk6AMNvvV"
      },
      "outputs": [],
      "source": [
        "def get_parameters(net) -> List[np.ndarray]:\n",
        "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
        "\n",
        "def set_parameters(net, parameters: List[np.ndarray]):\n",
        "    params_dict = zip(net.state_dict().keys(), parameters)\n",
        "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "    net.load_state_dict(state_dict, strict=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FLClient(fl.client.NumPyClient):\n",
        "        def __init__(self, net, trainloader, testloader):\n",
        "            self.net = net\n",
        "            self.trainloader = trainloader\n",
        "            self.testloader = testloader\n",
        "     \n",
        "        def get_parameters(self, config):\n",
        "            return get_parameters(self.net)\n",
        "\n",
        "        def fit(self, parameters, config):\n",
        "            set_parameters(self.net, parameters)\n",
        "            train(self.net, self.trainloader, epochs=1)\n",
        "            return get_parameters(self.net), len(self.trainloader), {}\n",
        "\n",
        "        def evaluate(self, parameters, config):\n",
        "            set_parameters(self.net, parameters)\n",
        "            loss, accuracy = test(self.net, self.testloader)\n",
        "            return float(loss), len(self.testloader), {\"accuracy\": float(accuracy)}"
      ],
      "metadata": {
        "id": "oeB202gPjkpt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clientID(id):\n",
        "    if id == \"1\":\n",
        "        return r\"/content/enron1.csv\"\n",
        "    if id == \"2\":\n",
        "        return r\"/content/enron2.csv\"\n",
        "    if id == \"3\":\n",
        "        return r\"/content/enron3.csv\"\n",
        "    if id == \"4\":\n",
        "        return r\"/content/enron4.csv\"\n",
        "    if id == \"5\":\n",
        "        return r\"/content/enron5.csv\"\n",
        "    if id == \"6\":\n",
        "        return r\"/content/enron6.csv\"\n",
        "            \n",
        "            \n",
        "def testID(id):\n",
        "    if id == \"1\":\n",
        "        return r\"/content/enron1.csv\"\n",
        "    if id == \"2\":\n",
        "        return r\"/content/enron2.csv\"\n",
        "    if id == \"3\":\n",
        "        return r\"/content/enron3.csv\"\n",
        "    if id == \"4\":\n",
        "        return r\"/content/enron4.csv\"\n",
        "    if id == \"5\":\n",
        "        return r\"/content/enron5.csv\"\n",
        "    if id == \"6\":\n",
        "        return r\"/content/enron6.csv\""
      ],
      "metadata": {
        "id": "9k11zKo8jkz1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(cid):\n",
        "    csvFile = clientID(cid)\n",
        "    \n",
        "    df = pd.read_csv(csvFile)\n",
        "\n",
        "    raw_datasets = Dataset.from_pandas(df)\n",
        "\n",
        "    raw_datasets = raw_datasets.shuffle(seed=42)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"message\"], truncation=True)\n",
        "\n",
        "    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "\n",
        "    tokenized_datasets = tokenized_datasets.remove_columns(\"message\")\n",
        "    tokenized_datasets = tokenized_datasets.rename_column(\"spam/ham\", \"labels\")\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    trainloader = DataLoader(\n",
        "        tokenized_datasets,\n",
        "        shuffle=True,\n",
        "        batch_size=32,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "    csvFile = testID(cid)\n",
        "    \n",
        "    df = pd.read_csv(csvFile)\n",
        "\n",
        "    raw_datasets = Dataset.from_pandas(df)\n",
        "\n",
        "    raw_datasets = raw_datasets.shuffle(seed=42)\n",
        "    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "    tokenized_datasets = tokenized_datasets.remove_columns(\"message\")\n",
        "    tokenized_datasets = tokenized_datasets.rename_column(\"spam/ham\", \"labels\")\n",
        "\n",
        "\n",
        "    testloader = DataLoader(\n",
        "        tokenized_datasets, batch_size=32, collate_fn=data_collator\n",
        "    )\n",
        "\n",
        "    return trainloader, testloader"
      ],
      "metadata": {
        "id": "PdOfDTmUjs_m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, trainloader, epochs):\n",
        "    optimizer = AdamW(net.parameters())\n",
        "    net.train()\n",
        "    for _ in range(epochs):\n",
        "        for batch in trainloader:\n",
        "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "            outputs = net(**batch)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "\n",
        "def test(net, testloader):\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    loss = 0\n",
        "    net.eval()\n",
        "    for batch in testloader:\n",
        "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = net(**batch)\n",
        "        logits = outputs.logits\n",
        "        loss += outputs.loss.item()\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "    loss /= len(testloader.dataset)\n",
        "    accuracy = metric.compute()[\"accuracy\"]\n",
        "    return loss, accuracy"
      ],
      "metadata": {
        "id": "Xe48d7vFjtH1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def client_fn(cid: str) -> FLClient:\n",
        "    # Create model\n",
        "    net = AutoModelForSequenceClassification.from_pretrained(\n",
        "        CHECKPOINT, num_labels=2\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    trainloader, testloader = load_data(cid)\n",
        "\n",
        "    # Create and return client\n",
        "    return FLClient(net, trainloader, testloader)"
      ],
      "metadata": {
        "id": "qqhnt0ZBjtOd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strategy = fl.server.strategy.FedAvg(\n",
        "        fraction_fit=1.0,  # Sample 100% of available clients for training\n",
        "        fraction_evaluate=0.5,  # Sample 50% of available clients for evaluation\n",
        ")\n",
        "\n",
        "\n",
        "fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=1),\n",
        "    strategy=strategy,\n",
        "    clients_ids = [\"1\",\"2\"],\n",
        ")\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14aOQN5jf6FK",
        "outputId": "b6c6d845-77bb-4b87-9b67-7e583a3fdb9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO flower 2022-08-02 22:21:30,933 | app.py:145 | Starting Flower simulation, config: ServerConfig(num_rounds=1, round_timeout=None)\n",
            "INFO:flower:Starting Flower simulation, config: ServerConfig(num_rounds=1, round_timeout=None)\n",
            "INFO flower 2022-08-02 22:21:37,571 | app.py:179 | Flower VCE: Ray initialized with resources: {'object_store_memory': 8059363737.0, 'memory': 16118727476.0, 'node:172.28.0.2': 1.0, 'CPU': 4.0}\n",
            "INFO:flower:Flower VCE: Ray initialized with resources: {'object_store_memory': 8059363737.0, 'memory': 16118727476.0, 'node:172.28.0.2': 1.0, 'CPU': 4.0}\n",
            "INFO flower 2022-08-02 22:21:37,579 | server.py:86 | Initializing global parameters\n",
            "INFO:flower:Initializing global parameters\n",
            "INFO flower 2022-08-02 22:21:37,582 | server.py:270 | Requesting initial parameters from one random client\n",
            "INFO:flower:Requesting initial parameters from one random client\n",
            "\u001b[2m\u001b[36m(launch_and_get_parameters pid=2340)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
            "\u001b[2m\u001b[36m(launch_and_get_parameters pid=2340)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(launch_and_get_parameters pid=2340)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(launch_and_get_parameters pid=2340)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "\u001b[2m\u001b[36m(launch_and_get_parameters pid=2340)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[2m\u001b[36m(launch_and_get_parameters pid=2340)\u001b[0m Parameter 'function'=<function load_data.<locals>.tokenize_function at 0x7fea9dc37830> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "  0%|          | 0/6 [00:00<?, ?ba/s]\n",
            " 17%|█▋        | 1/6 [00:00<00:02,  1.68ba/s]\n",
            " 33%|███▎      | 2/6 [00:01<00:02,  1.65ba/s]\n",
            " 50%|█████     | 3/6 [00:01<00:01,  1.68ba/s]\n",
            " 67%|██████▋   | 4/6 [00:02<00:01,  1.71ba/s]\n",
            " 83%|████████▎ | 5/6 [00:03<00:00,  1.64ba/s]\n",
            "100%|██████████| 6/6 [00:03<00:00,  1.71ba/s]\n",
            "  0%|          | 0/6 [00:00<?, ?ba/s]\n",
            " 17%|█▋        | 1/6 [00:00<00:02,  1.77ba/s]\n",
            " 33%|███▎      | 2/6 [00:01<00:02,  1.68ba/s]\n",
            " 50%|█████     | 3/6 [00:01<00:01,  1.68ba/s]\n",
            " 67%|██████▋   | 4/6 [00:02<00:01,  1.73ba/s]\n",
            " 83%|████████▎ | 5/6 [00:02<00:00,  1.66ba/s]\n",
            "100%|██████████| 6/6 [00:03<00:00,  1.72ba/s]\n",
            "INFO flower 2022-08-02 22:21:55,988 | server.py:274 | Received initial parameters from one random client\n",
            "INFO:flower:Received initial parameters from one random client\n",
            "INFO flower 2022-08-02 22:21:55,999 | server.py:88 | Evaluating initial parameters\n",
            "INFO:flower:Evaluating initial parameters\n",
            "INFO flower 2022-08-02 22:21:56,002 | server.py:101 | FL starting\n",
            "INFO:flower:FL starting\n",
            "DEBUG flower 2022-08-02 22:21:56,005 | server.py:220 | fit_round 1: strategy sampled 2 clients (out of 2)\n",
            "DEBUG:flower:fit_round 1: strategy sampled 2 clients (out of 2)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2340)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2340)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2340)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2340)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2340)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0%|          | 0/6 [00:00<?, ?ba/s]\n",
            " 17%|█▋        | 1/6 [00:00<00:03,  1.31ba/s]\n",
            " 33%|███▎      | 2/6 [00:01<00:02,  1.35ba/s]\n",
            " 50%|█████     | 3/6 [00:02<00:02,  1.32ba/s]\n",
            " 67%|██████▋   | 4/6 [00:02<00:01,  1.37ba/s]\n",
            " 83%|████████▎ | 5/6 [00:03<00:00,  1.50ba/s]\n",
            "100%|██████████| 6/6 [00:03<00:00,  1.67ba/s]\n",
            "  0%|          | 0/6 [00:00<?, ?ba/s]\n",
            " 17%|█▋        | 1/6 [00:00<00:02,  1.93ba/s]\n",
            " 33%|███▎      | 2/6 [00:01<00:02,  1.88ba/s]\n",
            " 50%|█████     | 3/6 [00:01<00:01,  1.91ba/s]\n",
            " 67%|██████▋   | 4/6 [00:02<00:01,  1.96ba/s]\n",
            "100%|██████████| 6/6 [00:02<00:00,  2.24ba/s]\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2341)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2341)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2341)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2341)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2341)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2341)\u001b[0m Parameter 'function'=<function load_data.<locals>.tokenize_function at 0x7faef1441830> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "  0%|          | 0/6 [00:00<?, ?ba/s]\n",
            " 17%|█▋        | 1/6 [00:00<00:03,  1.31ba/s]\n",
            " 33%|███▎      | 2/6 [00:01<00:03,  1.30ba/s]\n",
            " 50%|█████     | 3/6 [00:02<00:02,  1.33ba/s]\n",
            " 67%|██████▋   | 4/6 [00:03<00:01,  1.34ba/s]\n",
            " 83%|████████▎ | 5/6 [00:03<00:00,  1.25ba/s]\n",
            "100%|██████████| 6/6 [00:04<00:00,  1.31ba/s]\n",
            "  0%|          | 0/6 [00:00<?, ?ba/s]\n",
            " 17%|█▋        | 1/6 [00:00<00:03,  1.31ba/s]\n",
            " 33%|███▎      | 2/6 [00:01<00:03,  1.28ba/s]\n",
            " 50%|█████     | 3/6 [00:02<00:02,  1.27ba/s]\n",
            " 67%|██████▋   | 4/6 [00:03<00:01,  1.34ba/s]\n",
            " 83%|████████▎ | 5/6 [00:03<00:00,  1.30ba/s]\n",
            "100%|██████████| 6/6 [00:04<00:00,  1.32ba/s]\n",
            "2022-08-02 22:22:54,384\tWARNING worker.py:1404 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 63c98cba6b6c646fbdb9a2952be405563914545b01000000 Worker ID: 3c89294f4445e008abddd1bc02cdd28f4a4f7e0ebac233a08521d202 Node ID: fbf1a4d8adb8ce4e72892938769a7cf25024a5b8d7bac0dad08d3fb8 Worker IP address: 172.28.0.2 Worker port: 34403 Worker PID: 2340\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2338)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2338)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2338)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2338)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2338)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2338)\u001b[0m Parameter 'function'=<function load_data.<locals>.tokenize_function at 0x7f8b5314df80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "  0%|          | 0/6 [00:00<?, ?ba/s]\n",
            " 17%|█▋        | 1/6 [00:00<00:03,  1.58ba/s]\n",
            " 33%|███▎      | 2/6 [00:01<00:02,  1.61ba/s]\n",
            " 50%|█████     | 3/6 [00:01<00:01,  1.68ba/s]\n",
            " 67%|██████▋   | 4/6 [00:02<00:01,  1.72ba/s]\n",
            " 83%|████████▎ | 5/6 [00:03<00:00,  1.66ba/s]\n",
            "100%|██████████| 6/6 [00:03<00:00,  1.92ba/s]\n",
            "  0%|          | 0/6 [00:00<?, ?ba/s]\n",
            " 17%|█▋        | 1/6 [00:00<00:02,  1.71ba/s]\n",
            " 33%|███▎      | 2/6 [00:01<00:02,  1.74ba/s]\n",
            " 50%|█████     | 3/6 [00:01<00:02,  1.50ba/s]\n",
            " 67%|██████▋   | 4/6 [00:02<00:01,  1.62ba/s]\n",
            " 83%|████████▎ | 5/6 [00:03<00:00,  1.70ba/s]\n",
            "100%|██████████| 6/6 [00:03<00:00,  1.94ba/s]\n",
            "2022-08-02 22:23:27,617\tWARNING worker.py:1404 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 1a202a6b0c54d6a1943f8610b81c3a3d9a296b4e01000000 Worker ID: 028153643da7183add1a16c035302903ca4e914adb4c38af9dd4fafd Node ID: fbf1a4d8adb8ce4e72892938769a7cf25024a5b8d7bac0dad08d3fb8 Worker IP address: 172.28.0.2 Worker port: 45295 Worker PID: 2341\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2339)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2339)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2339)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2339)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2339)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=2339)\u001b[0m Parameter 'function'=<function load_data.<locals>.tokenize_function at 0x7f6bf76fe830> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "  0%|          | 0/6 [00:00<?, ?ba/s]\n",
            " 17%|█▋        | 1/6 [00:00<00:03,  1.35ba/s]\n",
            " 33%|███▎      | 2/6 [00:01<00:03,  1.27ba/s]\n",
            " 50%|█████     | 3/6 [00:02<00:02,  1.31ba/s]\n",
            " 67%|██████▋   | 4/6 [00:02<00:01,  1.37ba/s]\n",
            " 83%|████████▎ | 5/6 [00:03<00:00,  1.28ba/s]\n",
            "100%|██████████| 6/6 [00:04<00:00,  1.35ba/s]\n",
            "  0%|          | 0/6 [00:00<?, ?ba/s]\n",
            " 17%|█▋        | 1/6 [00:00<00:03,  1.37ba/s]\n",
            " 33%|███▎      | 2/6 [00:01<00:03,  1.30ba/s]\n",
            " 50%|█████     | 3/6 [00:02<00:02,  1.32ba/s]\n",
            " 67%|██████▋   | 4/6 [00:03<00:01,  1.30ba/s]\n",
            " 83%|████████▎ | 5/6 [00:03<00:00,  1.25ba/s]\n",
            "100%|██████████| 6/6 [00:04<00:00,  1.30ba/s]\n"
          ]
        }
      ]
    }
  ]
}